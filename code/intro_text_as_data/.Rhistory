require(quanteda)
summary(data_corpus_inaugural)
summary(data_corpus_inaugural[1:5])
data_corpus_inaugural[1]
cat(data_corpus_inaugural[2])
ndoc(data_corpus_inaugural)
docnames(data_corpus_inaugural)
nchar(data_corpus_inaugural[1:7])
ntoken(data_corpus_inaugural[1:7])
ntoken(data_corpus_inaugural[1:7], remove_punct = TRUE)
ntype(data_corpus_inaugural[1:7])
?tokens
tokens("Today is Thursday in Canberra. It is yesterday in London.")
vec <- c(one = "This is text one",
two = "This, however, is the second text")
tokens(vec)
tokens(char_tolower(vec), remove_punct = TRUE)
inaugTokens <- tokens(data_corpus_inaugural, remove_punct = TRUE)
tokens_tolower(inaugTokens[2])
inaugDfm <- dfm(inaugTokens)
trimmedInaugDfm <- dfm_trim(inaugDfm, min_doc = 5, min_count = 10)
weightedTrimmedDfm <- dfm_tfidf(trimmedInaugDfm)
require(magrittr)
inaugDfm2 <- dfm(inaugTokens) %>%
dfm_trim(min_doc = 5, min_count = 10) %>%
dfm_tfidf()
methods(dfm)
methods(class = "tokens")
summary(data_corpus_inaugural[52:57])
dv <- data.frame(Party = c("dem", "dem", "rep", "rep", "dem", "dem"))
recentCorpus <- corpus(data_corpus_inaugural[52:57], docvars = dv)
summary(recentCorpus)
partyDfm <- dfm(recentCorpus, groups = "Party", remove = (stopwords("english")))
textplot_wordcloud(partyDfm, comparison = TRUE)
require(quanteda)
s1 <- 'my example text'
length(s1)
nchar(s1)
s2 <- c('This is', 'my example text.', 'So imaginative.')
length(s2)
nchar(s2)
sum(nchar(s2))
inaugTexts <- texts(data_corpus_inaugural)
which.max(nchar(inaugTexts))
which.min(nchar(inaugTexts))
s1 <- 'This file contains many fascinating example sentences.'
s1[6:9]
s1 <- 'This file contains many fascinating example sentences.'
substr(s1, 6,9)
names(inaugTexts)
# returns a list of parts
s1 <- 'split this string'
strsplit(s1, 'this')
parts <- strsplit(names(inaugTexts), '-')
years <- sapply(parts, function(x) x[1])
pres <-  sapply(parts, function(x) x[2])
paste('one', 'two', 'three')
paste('one', 'two', 'three', sep = '_')
paste(years, pres, sep = '-')
paste(years, pres, collapse = '-')
tolower(s1)
toupper(s1)
tolower(s1) == toupper(s1)
'apples'=='oranges'
tolower(s1) == tolower(s1)
'pears' == 'pears'
c1 <- c('apples', 'oranges', 'pears')
'pears' %in% c1
c2 <- c('bananas', 'pears')
c2 %in% c1
?grep
grep('orangef', 'these are oranges')
grep('pear', 'these are oranges')
grep('orange', c('apples', 'oranges', 'pears'))
grep('pears', c('apples', 'oranges', 'pears'))
gsub('oranges', 'apples', 'these are oranges')
require(stringr)
pattern <- "a.b"
strings <- c("abb", "a.b")
str_detect(strings, pattern)
str_extract_all("The Cat in the Hat", "[a-z]+")
str_extract_all("The Cat in the Hat", regex("[a-z]+", TRUE))
str_extract_all("a\nb\nc", "^.")
str_extract_all("a\nb\nc", regex("^.", multiline = TRUE))
fruits <- c("one apple", "two pears", "three bananas")
str_replace(fruits, "[aeiou]", "-")
str_replace_all(fruits, "[aeiou]", "-")
str_replace(fruits, "([aeiou])", "")
str_replace(fruits, "([aeiou])", "\\1\\1")
str_replace(fruits, "[aeiou]", c("1", "2", "3"))
str_replace(fruits, c("a", "e", "i"), "-")
str_trim("  String with trailing and leading white space\t")
str_trim("\n\nString with trailing and leading white space\n\n")
# devtools packaged required to install readtext from Github
devtools::install_github("kbenoit/readtext")
require(quanteda, warn.conflicts = FALSE, quietly = TRUE)
require(readtext)
myCorpus <- corpus(readtext("inaugural/*.txt"))
require(quanteda, quietly = TRUE, warn.conflicts = FALSE)
txt <- c(text1 = "This is $10 in 999 different ways,\n up and down; left and right!",
text2 = "@kenbenoit working: on #quanteda 2day\t4ever, http://textasdata.com?page=123.")
tokens(txt)
tokens(txt, verbose = TRUE)
tokens(txt, remove_numbers = TRUE,  remove_punct = TRUE)
tokens(txt, remove_numbers = FALSE, remove_punct = TRUE)
tokens(txt, remove_numbers = TRUE,  remove_punct = FALSE)
tokens(txt, remove_numbers = FALSE, remove_punct = FALSE)
tokens(txt, remove_numbers = FALSE, remove_punct = FALSE, remove_separators = FALSE)
# sentence level
tokens(c("Kurt Vongeut said; only assholes use semi-colons.",
"Today is Thursday in Canberra:  It is yesterday in London.",
"Today is Thursday in Canberra:  \nIt is yesterday in London.",
"To be?  Or\nnot to be?"),
what = "sentence")
tokens(data_corpus_inaugural[2], what = "sentence")
# character level
tokens("My big fat text package.", what = "character")
tokens("My big fat text package.", what = "character", remove_separators = FALSE)
# reading data and computing additional variables
library(quanteda)
library(quanteda.corpora)
# saveing corpus object from quanteda.corpora
data_corpus_SOTU<-data_corpus_sotu
summary(data_corpus_SOTU)
SOTU_stat <- textstat_readability(data_corpus_SOTU)
SOTU_stat
View(SOTU_stat)
# readability
measure <- "Flesch"
SOTU_stat <- textstat_readability(data_corpus_SOTU, measure)
SOTU_year <- lubridate::year(docvars(data_corpus_SOTU, "Date"))
SOTU_df <- data.frame("year" = SOTU_year, "SOTU_stat" = SOTU_stat$Flesch)
plot(SOTU_df)
# lexical diversity -- Type to Token ratio
SOTU_stat <- textstat_lexdiv(dfm(data_corpus_SOTU) )
SOTU_year <- lubridate::year(docvars(data_corpus_SOTU, "Date"))
SOTU_df <- data.frame("year" = SOTU_year, "SOTU_stat" = SOTU_stat$TTR)
plot(SOTU_df)
rew <- dfm_tfidf(dfm(data_corpus_SOTU))
# now most frequent features are different
topfeatures(dfm(data_corpus_SOTU))
topfeatures(rew)
dfm_sotu<-dfm(data_corpus_SOTU)
?textstat_simil
simils <- textstat_simil(dfm_sotu, "Trump-2018", margin="documents", method="jaccard")
simils
df <- data.frame(
docname = rownames(simils),
simil = as.numeric(simils),
stringsAsFactors=F
)
order(simils)
?order
tail(df[order(simils),])
order(simils)
df[1,]
df[1,1]
tail(df[order(simils)])
head(df[order(simils),])
tail(df[order(simils),])
simils <- textstat_simil(dfm_sotu, "Obama-2012", margin="documents", method="jaccard")
df <- data.frame(
docname = rownames(simils),
simil = as.numeric(simils),
stringsAsFactors=F
)
tail(df[order(simils),])
head(df[order(simils),])
?tfidf
dfm_sotu<-dfm_tfidf(dfm(data_corpus_SOTU ))
simils <- textstat_simil(dfm_sotu, "Trump-2018", margin="documents", method="jaccard")
df <- data.frame(
docname = rownames(simils),
simil = as.numeric(simils),
stringsAsFactors=F
)
?order
tail(df[order(simils),])
head(df[order(simils),])
# term similarities
simils <- textstat_simil(dfm_sotu, "unemployment", margin="features", method="cosine")
# most similar features
df <- data.frame(
featname = rownames(simils),
simil = as.numeric(simils),
stringsAsFactors=F
)
head(df[order(simils, decreasing=TRUE),], n=10)
# another example...
simils <- textstat_simil(dfm_sotu, "america", margin="features", method="cosine")
# most similar features
df <- data.frame(
featname = rownames(simils),
simil = as.numeric(simils),
stringsAsFactors=F
)
head(df[order(simils, decreasing=TRUE),], n=10)
?textstat_simil
simils <- textstat_simil(dfm_sotu, "unemployment", margin="features", method="faith")
# term similarities
simils <- textstat_simil(dfm_sotu, "unemployment", margin="features", method="faith")
# most similar features
df <- data.frame(
featname = rownames(simils),
simil = as.numeric(simils),
stringsAsFactors=F
)
head(df[order(simils, decreasing=TRUE),], n=10)
# another example...
simils <- textstat_simil(dfm_sotu, "america", margin="features", method="cosine")
# most similar features
df <- data.frame(
featname = rownames(simils),
simil = as.numeric(simils),
stringsAsFactors=F
)
head(df[order(simils, decreasing=TRUE),], n=10)
recent <- dfm_sotu[190:239,]
distances <- textstat_dist(recent, margin="documents")
as.matrix(distances)[1:5, 1:5]
cluster <- hclust(distances)
plot(cluster)
?prcomp
pca <- prcomp(t(as.matrix(dfm_sotu)))
plot(pca) # first PC captures most of the variance
plot(pca$rotation[,1], pca$rotation[,2])
text(pca$rotation[,1], pca$rotation[,2], labels=lubridate::year(docvars(data_corpus_SOTU, "Date")))
head(df[order(df$dim1),])
df <- data.frame(
featname = featnames(dfm_sotu),
dim1 = pca$x[,1],
dim2 = pca$x[,2],
stringsAsFactors=FALSE
)
head(df[order(df$dim1),])
tail(df[order(df$dim1),])
head(df[order(df$dim2),])
tail(df[order(df$dim2),])
?tfidf
pca <- prcomp(t(as.matrix(dfm_tfidf(dfm_sotu))) )
out <- textmodel_ca(dfm_sotu)
library(readtext)
install.packages(readtext)
install.packages("readtext")
install.packages("readtext")
require(quanteda)
summary(data_corpus_inaugural)
?data_corpus_inaugural
